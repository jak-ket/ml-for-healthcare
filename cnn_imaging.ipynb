{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "sxxCKPZZ2Nru",
      "metadata": {
        "id": "sxxCKPZZ2Nru"
      },
      "source": [
        "# Notebook for Data Analysis and Integrated Gradients in Pneumonia Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dXPDAYs1sur",
      "metadata": {
        "id": "4dXPDAYs1sur"
      },
      "source": [
        "The model in dictionary form can be downloaded from https://polybox.ethz.ch/index.php/s/j5BofTKcxnnx39t\n",
        "\n",
        "The 10 images used are those that you can find here: https://polybox.ethz.ch/index.php/s/7eLrhKFe34UW3tW"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d100dce8-23b4-4c1a-993e-5055597a1f89",
      "metadata": {
        "id": "d100dce8-23b4-4c1a-993e-5055597a1f89"
      },
      "source": [
        "**Load the Required Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-24T10:50:35.576525Z",
          "start_time": "2024-03-24T10:50:34.276191Z"
        },
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# append the filepath to where torch is installed\n",
        "sys.path.append('/home/millerm/.local/lib/python3.10/site-packages')\n",
        "# sys.path.append('/home/username/.local/lib/python3.10/site-packages')\n",
        "\n",
        "import torch\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1bf4e59d82a7568",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-24T11:09:57.714324Z",
          "start_time": "2024-03-24T11:09:57.708654Z"
        },
        "id": "e1bf4e59d82a7568"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchinfo import summary\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import v2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97af3747-dd6a-47f2-86f5-52112d72c2c3",
      "metadata": {
        "id": "97af3747-dd6a-47f2-86f5-52112d72c2c3"
      },
      "source": [
        "We load the functions from pytorchcv. As you might experience complications importing the required pieces directly, we define the necessary functions separately below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9359b7a1-ae05-45b5-913e-9f20788d31c0",
      "metadata": {
        "id": "9359b7a1-ae05-45b5-913e-9f20788d31c0"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/computer-vision-pytorch/pytorchcv.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9b1f86683fc1550",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-24T11:15:57.381383Z",
          "start_time": "2024-03-24T11:15:57.117087Z"
        },
        "id": "c9b1f86683fc1550"
      },
      "outputs": [],
      "source": [
        "from pytorchcv import train, plot_results, display_dataset, train_long"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "831575ba-11e3-4fb3-8b21-4af3dfc8dd8d",
      "metadata": {
        "id": "831575ba-11e3-4fb3-8b21-4af3dfc8dd8d"
      },
      "source": [
        "**Load the Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3fba85c-7efd-4c4e-9496-100548b437c2",
      "metadata": {
        "id": "c3fba85c-7efd-4c4e-9496-100548b437c2"
      },
      "source": [
        "For this notebook, please refer to the model 20_model_state.pth. Unfortunately, we have been unable to load the full model in the student cluster such that we only provide the dictionary solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d9126981eb0accb",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-24T10:57:04.164486Z",
          "start_time": "2024-03-24T10:57:02.321018Z"
        },
        "id": "5d9126981eb0accb"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import VGG16_Weights\n",
        "model = torchvision.models.vgg16(weights=VGG16_Weights.DEFAULT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "282b6309-0a12-4c98-bed0-a937b76bd93e",
      "metadata": {
        "id": "282b6309-0a12-4c98-bed0-a937b76bd93e"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print('Doing computations on device = {}'.format(device))\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8867f03-11be-4ed6-812b-7b4cd0101ad8",
      "metadata": {
        "id": "d8867f03-11be-4ed6-812b-7b4cd0101ad8"
      },
      "outputs": [],
      "source": [
        "model.classifier = nn.Sequential(\n",
        "    torch.nn.Linear(25088,4096),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Dropout(0.5, inplace = False),\n",
        "    torch.nn.Linear(4096,4096),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Dropout(0.5, inplace = False),\n",
        "    torch.nn.Linear(4096,4096),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Dropout(0.5, inplace = False),\n",
        "    torch.nn.Linear(4096,4096),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Dropout(0.5, inplace = False),\n",
        "    torch.nn.Linear(4096,2)\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50570c03-0c9f-4f3a-ad01-3201bdc4f3f6",
      "metadata": {
        "id": "50570c03-0c9f-4f3a-ad01-3201bdc4f3f6"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('models/20_model_state.pth'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2948c618-17d0-407f-afe1-a7e3fb90ff3e",
      "metadata": {
        "id": "2948c618-17d0-407f-afe1-a7e3fb90ff3e"
      },
      "source": [
        "**Transform and Visualize the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e93b25cf-3d25-4881-8c42-58a8a9a1318d",
      "metadata": {
        "id": "e93b25cf-3d25-4881-8c42-58a8a9a1318d"
      },
      "outputs": [],
      "source": [
        "trans_wo_norm = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset0_wo_norm = torchvision.datasets.ImageFolder(\"ml4h_data/project1/chest_xray/train/\", transform=trans_wo_norm)\n",
        "dataset1_wo_norm = torchvision.datasets.ImageFolder(\"ml4h_data/project1/chest_xray/test/\", transform=trans_wo_norm)\n",
        "dataset2_wo_norm = torchvision.datasets.ImageFolder(\"ml4h_data/project1/chest_xray/val/\", transform=trans_wo_norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "125f4c3f-6615-4ef9-a87b-ad48bb304b31",
      "metadata": {
        "id": "125f4c3f-6615-4ef9-a87b-ad48bb304b31"
      },
      "source": [
        "We compute the mean and standard deviation using the following snippet. However, this will take a few minutes such that we have included the hard-coded valuesfor your convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4feac1f0-bfba-49c3-b257-85e842bb15ff",
      "metadata": {
        "id": "4feac1f0-bfba-49c3-b257-85e842bb15ff"
      },
      "outputs": [],
      "source": [
        "# mean0 = torch.zeros(3)\n",
        "# std0 = torch.zeros(3)\n",
        "# for img, _ in dataset0_wo_norm:\n",
        "#     mean0 += img.mean(dim=(1, 2))\n",
        "#     std0 += img.std(dim=(1, 2))\n",
        "\n",
        "# mean0 /= len(dataset0_wo_norm)\n",
        "# std0 /= len(dataset0_wo_norm)\n",
        "\n",
        "# print(\"Mean:\", mean0)\n",
        "# print(\"Standard deviation:\", std0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a2a6c98-bbe9-4d9e-a484-4caec584fe24",
      "metadata": {
        "id": "5a2a6c98-bbe9-4d9e-a484-4caec584fe24"
      },
      "outputs": [],
      "source": [
        "mean0 = torch.tensor([0.5832, 0.5832, 0.5832])\n",
        "std0  = torch.tensor([0.1413, 0.1413, 0.1413])\n",
        "mean1 = torch.tensor([0.5763, 0.5763, 0.5763])\n",
        "std1  = torch.tensor([0.1453, 0.1453, 0.1453])\n",
        "mean2 = torch.tensor([0.6020, 0.6020, 0.6020])\n",
        "std2  = torch.tensor([0.1401, 0.1401, 0.1401])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "413b3674b78327a1",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-24T11:07:47.529112Z",
          "start_time": "2024-03-24T11:07:47.503108Z"
        },
        "id": "413b3674b78327a1"
      },
      "outputs": [],
      "source": [
        "std_normalise_0 = transforms.Normalize(\n",
        "    mean=mean0,\n",
        "    std=std0\n",
        ")\n",
        "std_normalise_1 = transforms.Normalize(\n",
        "    mean=mean1,\n",
        "    std=std1\n",
        ")\n",
        "std_normalise_2 = transforms.Normalize(\n",
        "    mean=mean2,\n",
        "    std=std2\n",
        ")\n",
        "\n",
        "trans0 = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.ToTensor(),\n",
        "        std_normalise_0\n",
        "])\n",
        "trans1 = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.ToTensor()\n",
        "])\n",
        "trans2 = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "        transforms.ToTensor()\n",
        "])\n",
        "\n",
        "random_trans = v2.RandomOrder([\n",
        "        v2.GaussianBlur(3)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47fe11c02611918d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-24T12:27:46.893669Z",
          "start_time": "2024-03-24T12:27:46.861519Z"
        },
        "id": "47fe11c02611918d"
      },
      "outputs": [],
      "source": [
        "dataset_0 = torchvision.datasets.ImageFolder(\"ml4h_data/project1/chest_xray/train/\", transform=trans0)\n",
        "dataset_1 = torchvision.datasets.ImageFolder(\"ml4h_data/project1/chest_xray/test\", transform=trans1)\n",
        "dataset_2 = torchvision.datasets.ImageFolder(\"ml4h_data/project1/chest_xray/val\", transform=trans2)\n",
        "\n",
        "dataset_0 = random_trans(dataset_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98e630471737f3bb",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-24T12:21:42.334808Z",
          "start_time": "2024-03-24T12:21:42.328817Z"
        },
        "id": "98e630471737f3bb"
      },
      "outputs": [],
      "source": [
        "def display_dataset(dataset, n=2,classes=('NORMAL','PNEUMONIA')):\n",
        "    fig,ax = plt.subplots(1,n,figsize=(15,3))\n",
        "    mn = min([dataset[i][0].min() for i in range(n)])\n",
        "    mx = max([dataset[i][0].max() for i in range(n)])\n",
        "    for i in range(n):\n",
        "        ax[i].imshow(np.transpose((dataset[i][0]-mn)/(mx-mn),(1,2,0)))\n",
        "        ax[i].axis('off')\n",
        "        if classes:\n",
        "            ax[i].set_title(classes[dataset[i][1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eac9226-3197-4381-a611-af8679da84a5",
      "metadata": {
        "id": "8eac9226-3197-4381-a611-af8679da84a5"
      },
      "outputs": [],
      "source": [
        "display_dataset(dataset_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40208b33-a71c-469a-8fa8-f4f3e238c4c0",
      "metadata": {
        "id": "40208b33-a71c-469a-8fa8-f4f3e238c4c0"
      },
      "source": [
        "**Train the Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aeb584dc-9bd6-438d-a326-f69b1c7e2d27",
      "metadata": {
        "id": "aeb584dc-9bd6-438d-a326-f69b1c7e2d27"
      },
      "source": [
        "Training takes a few hours. We have trained our model for 21 epochs to reach sensible results. Hence, you are advised to simply load the model as indicated above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74d5b4c4-649f-4a25-84a0-3ef6b098c05b",
      "metadata": {
        "id": "74d5b4c4-649f-4a25-84a0-3ef6b098c05b"
      },
      "outputs": [],
      "source": [
        "for param in model.features.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "257bc8be5ce45ca2",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-24T12:24:40.405273Z",
          "start_time": "2024-03-24T12:24:40.373574Z"
        },
        "id": "257bc8be5ce45ca2"
      },
      "outputs": [],
      "source": [
        "def train_long(net,train_loader,test_loader,epochs=5,lr=0.001,optimizer=None,loss_fn = nn.NLLLoss(),print_freq=10):\n",
        "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "        total_loss,acc,count = 0,0,0\n",
        "        for i, (features,labels) in enumerate(train_loader):\n",
        "            lbls = labels.to(default_device)\n",
        "            optimizer.zero_grad()\n",
        "            out = net(features.to(default_device))\n",
        "            loss = loss_fn(out,lbls)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss+=loss\n",
        "            _,predicted = torch.max(out,1)\n",
        "            acc+=(predicted==lbls).sum()\n",
        "            count+=len(labels)\n",
        "            if i%print_freq==0:\n",
        "                print(\"Epoch {}, minibatch {}: train acc = {}, train loss = {}\".format(epoch,i,acc.item()/count,total_loss.item()/count))\n",
        "        vl,va = validate(net,test_loader,loss_fn)\n",
        "        print(\"Epoch {} done, validation acc = {}, validation loss = {}\".format(epoch,va,vl))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29272c2c-cede-4bd5-b9af-89fa9da5b2d4",
      "metadata": {
        "id": "29272c2c-cede-4bd5-b9af-89fa9da5b2d4"
      },
      "outputs": [],
      "source": [
        "def validate(net, dataloader,loss_fn=nn.NLLLoss()):\n",
        "    net.eval()\n",
        "    count,acc,loss = 0,0,0\n",
        "    with torch.no_grad():\n",
        "        for features,labels in dataloader:\n",
        "            lbls = labels.to(default_device)\n",
        "            out = net(features.to(default_device))\n",
        "            loss += loss_fn(out,lbls)\n",
        "            pred = torch.max(out,1)[1]\n",
        "            acc += (pred==lbls).sum()\n",
        "            count += len(labels)\n",
        "    return loss.item()/count, acc.item()/count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f85ccc9-eaff-4801-97ad-cf89b1b9d329",
      "metadata": {
        "id": "0f85ccc9-eaff-4801-97ad-cf89b1b9d329"
      },
      "outputs": [],
      "source": [
        "num_samples = 3500\n",
        "torch.manual_seed(1234)\n",
        "trainset, testset = torch.utils.data.random_split(dataset_0, [num_samples, len(dataset_0) - num_samples])\n",
        "train_loader = torch.utils.data.DataLoader(trainset,batch_size=32)\n",
        "test_loader  = torch.utils.data.DataLoader(testset,batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2eb8c00366f7f2e",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-03-24T12:29:48.106126Z"
        },
        "id": "f2eb8c00366f7f2e",
        "is_executing": true
      },
      "outputs": [],
      "source": [
        "# default_device = device\n",
        "# train_long(model,train_loader,test_loader,lr=0.00001,loss_fn=torch.nn.CrossEntropyLoss(),epochs=21,print_freq=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "040739ae-afbf-4316-bccd-e68f21e3a73b",
      "metadata": {
        "id": "040739ae-afbf-4316-bccd-e68f21e3a73b"
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), '20_model_state.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01e29dca-6b54-4ef8-b646-a35bc458daf6",
      "metadata": {
        "id": "01e29dca-6b54-4ef8-b646-a35bc458daf6"
      },
      "source": [
        "**Evaluate the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1e3e98d-3517-4242-9f10-7cfbfc2341a7",
      "metadata": {
        "id": "c1e3e98d-3517-4242-9f10-7cfbfc2341a7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7c4e937-26d1-4b0f-8f0d-bd436aa74442",
      "metadata": {
        "id": "b7c4e937-26d1-4b0f-8f0d-bd436aa74442"
      },
      "outputs": [],
      "source": [
        "def match_label_to_folder(image_path):\n",
        "    parent_folder = os.path.basename(os.path.dirname(image_path))\n",
        "    image_filename = os.path.splitext(os.path.basename(image_path))[0]\n",
        "    label_name = parent_folder\n",
        "    return label_name, image_filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29dfe701-ed97-415e-a14d-9e65a33db40f",
      "metadata": {
        "id": "29dfe701-ed97-415e-a14d-9e65a33db40f"
      },
      "outputs": [],
      "source": [
        "test_file = 'ml4h_data/project1/chest_xray/test/NORMAL/IM-0033-0001-0001.jpeg' # Visualize a test file\n",
        "test_img = Image.open(test_file)\n",
        "\n",
        "image_path = test_file\n",
        "label_name, image_filename = match_label_to_folder(image_path)\n",
        "print(\"Label Name:\", label_name)\n",
        "print(\"Image Filename:\", image_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a899a6b-10e7-4d5f-b780-421f1df7a27e",
      "metadata": {
        "id": "3a899a6b-10e7-4d5f-b780-421f1df7a27e"
      },
      "outputs": [],
      "source": [
        "predictions_NORMAL = []\n",
        "data_folder = \"ml4h_data/project1/chest_xray/test/NORMAL\"\n",
        "\n",
        "for filename in os.listdir(data_folder):\n",
        "    if filename.endswith(\".jpeg\"):\n",
        "        image_path = os.path.join(data_folder, filename)\n",
        "        image = Image.open(image_path)\n",
        "        transformed_img = trans1(image).to(device)\n",
        "        input_img = std_normalise_1(transformed_img)\n",
        "        input_img = input_img.unsqueeze(0)\n",
        "        input_img.to(device)\n",
        "\n",
        "        outputs = model(input_img)\n",
        "        output = F.softmax(outputs, dim=1)\n",
        "        prediction_score, pred_label_idx = torch.topk(output, 1)\n",
        "        predicted_label = \"PNEUMONIA\" if pred_label_idx == 1 else \"NORMAL\"\n",
        "\n",
        "        true_label = match_label_to_folder(image_path)[0]\n",
        "        true_label_idx = 1 if true_label == \"PNEUMONIA\" else 0\n",
        "\n",
        "        predictions_NORMAL.append(pred_label_idx.item())\n",
        "\n",
        "predictions_PNEUMONIA = []\n",
        "data_folder = \"ml4h_data/project1/chest_xray/test/PNEUMONIA\"\n",
        "\n",
        "for filename in os.listdir(data_folder):\n",
        "    if filename.endswith(\".jpeg\"):\n",
        "        image_path = os.path.join(data_folder, filename)\n",
        "        image = Image.open(image_path)\n",
        "        transformed_img = trans1(image).to(device)\n",
        "        input_img = std_normalise_1(transformed_img)\n",
        "        input_img = input_img.unsqueeze(0)\n",
        "        input_img.to(device)\n",
        "\n",
        "        outputs = model(input_img)\n",
        "        output = F.softmax(outputs, dim=1)\n",
        "        prediction_score, pred_label_idx = torch.topk(output, 1)\n",
        "        predicted_label = \"PNEUMONIA\" if pred_label_idx == 1 else \"NORMAL\"\n",
        "\n",
        "        true_label = match_label_to_folder(image_path)[0]\n",
        "        true_label_idx = 1 if true_label == \"PNEUMONIA\" else 0\n",
        "\n",
        "        predictions_PNEUMONIA.append(pred_label_idx.item())\n",
        "\n",
        "(sum(predictions_PNEUMONIA)+len(predictions_NORMAL)-sum(predictions_NORMAL))/(len(predictions_PNEUMONIA)+len(predictions_NORMAL))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "011e7ae9-1cb0-4bc0-946a-1dd2c006de07",
      "metadata": {
        "id": "011e7ae9-1cb0-4bc0-946a-1dd2c006de07"
      },
      "source": [
        "**Run Integrated Gradients**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3777aa00-b0c3-4167-a279-d9b5086daaa7",
      "metadata": {
        "id": "3777aa00-b0c3-4167-a279-d9b5086daaa7"
      },
      "outputs": [],
      "source": [
        "import captum\n",
        "from captum.attr import IntegratedGradients\n",
        "from captum.attr import visualization as viz\n",
        "from matplotlib.colors import LinearSegmentedColormap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ca9c4d9-e95f-4a73-b294-6ebba33857f3",
      "metadata": {
        "id": "1ca9c4d9-e95f-4a73-b294-6ebba33857f3"
      },
      "outputs": [],
      "source": [
        "channels = 3\n",
        "height = 224\n",
        "width = 224\n",
        "batch_size = 1\n",
        "\n",
        "black_image = torch.zeros((batch_size, channels, height, width))\n",
        "black_image = black_image.to(device)\n",
        "\n",
        "white_image = torch.ones((batch_size, channels, height, width)) * 255\n",
        "white_image = white_image.to(device)\n",
        "\n",
        "pink_image = torch.zeros((batch_size, channels, height, width))\n",
        "pink_image[:, 0, :, :] = 255\n",
        "pink_image[:, 1, :, :] = 192\n",
        "pink_image[:, 2, :, :] = 203\n",
        "pink_image = pink_image.to(device)\n",
        "\n",
        "noisy_pixels = torch.randint(0, 256, (batch_size, channels, height, width))\n",
        "noisy_image = noisy_pixels.type(torch.FloatTensor)\n",
        "noisy_image = noisy_image.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3751e5e-cd6f-41ad-a286-7109a0f34661",
      "metadata": {
        "id": "d3751e5e-cd6f-41ad-a286-7109a0f34661"
      },
      "outputs": [],
      "source": [
        "default_cmap = LinearSegmentedColormap.from_list('zurichblue',\n",
        "                                                 [(0, '#ff8f4b'),\n",
        "                                                  (0.5, '#ffffff'),\n",
        "                                                  (1, '#0070b4')], N=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ed4d76a-ad18-45de-b638-4e0293fcc933",
      "metadata": {
        "id": "2ed4d76a-ad18-45de-b638-4e0293fcc933"
      },
      "source": [
        "Create a folder in your directory called \"images\" such that we can print the Integrated Gradients visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18d2367c-aa22-4015-a309-e788b41a3c1d",
      "metadata": {
        "id": "18d2367c-aa22-4015-a309-e788b41a3c1d"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "\n",
        "img_id = [6, 8, 9, 11, 12]\n",
        "# img_id = [33,35,39,69,70] # for selecting the NORMAL patients, use this code to print the images\n",
        "for id in img_id:\n",
        "    test_file = f\"img_for_saliency/PNEUMONIA/person1_virus_{id}.jpeg\"\n",
        "    # test_file = f\"img_for_saliency/NORMAL/IM-00{id}-0001.jpeg\"\n",
        "    # for selecting the NORMAL patients, use this code to print the images\n",
        "    test_img = Image.open(test_file)\n",
        "    transformed_img = trans1(test_img).to(device)\n",
        "    input_img = std_normalise_1(transformed_img)\n",
        "    input_img = input_img.unsqueeze(0)\n",
        "    input_img.to(device)\n",
        "    output = model(input_img)\n",
        "    output = F.softmax(output, dim=1)\n",
        "    true_label = match_label_to_folder(test_file)[0]\n",
        "    integrated_gradients = IntegratedGradients(model)\n",
        "    attributions_ig = integrated_gradients.attribute(input_img,\n",
        "                                                     target=true_label_idx,\n",
        "                                                     n_steps=100,\n",
        "                                                     baselines = black_image)\n",
        "    filename = f\"images/ig_{true_label}_{id}_perm.png\"\n",
        "\n",
        "    fig, _ = viz.visualize_image_attr(np.transpose(attributions_ig.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                                      np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                                      method='blended_heat_map',\n",
        "                                      cmap=default_cmap,\n",
        "                                      show_colorbar=True,\n",
        "                                      sign='all',\n",
        "                                      title='Integrated Gradients'\n",
        "                                     )\n",
        "\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf, format='png')\n",
        "    buf.seek(0)\n",
        "    img = Image.open(buf)\n",
        "    img.save(filename)\n",
        "\n",
        "    print(f\"Saved {filename}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
